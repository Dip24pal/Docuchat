{"arxiv_id": "2409.18048v1", "published": "2024-09-26T16:49:57Z", "title": "Next-Gen Software Engineering: AI-Assisted Big Models", "summary": "The effectiveness of model-driven software engineering (MDSE) has been\ndemonstrated in the context of complex software; however, it has not been\nwidely adopted due to the requisite efforts associated with model development\nand maintenance, as well as the specific modelling competencies required for\nMDSE. Concurrently, artificial intelligence (AI) methods, particularly machine\nlearning (ML) methods, have demonstrated considerable abilities when applied to\nthe huge code bases accessible on open-source coding platforms. The so-called\nbig code provides the basis for significant advances in empirical software\nengineering, as well as in the automation of coding processes and improvements\nin software quality with the use of AI. The objective of this paper is to\nfacilitate a synthesis between these two significant domains of software\nengineering (SE), namely models and AI in SE. The paper provides an overview of\nthe current status of AI-assisted software engineering. In light of the\naforementioned considerations, a vision of AI-assisted Big Models in SE is put\nforth, with the aim of capitalising on the advantages inherent to both\napproaches in the context of software development. Finally, the new paradigm of\npair modelling in MDSE is proposed."}
{"arxiv_id": "2409.17937v1", "published": "2024-09-26T15:12:41Z", "title": "Adaptive Stream Processing on Edge Devices through Active Inference", "summary": "The current scenario of IoT is witnessing a constant increase on the volume\nof data, which is generated in constant stream, calling for novel architectural\nand logical solutions for processing it. Moving the data handling towards the\nedge of the computing spectrum guarantees better distribution of load and, in\nprinciple, lower latency and better privacy. However, managing such a structure\nis complex, especially when requirements, also referred to Service Level\nObjectives (SLOs), specified by applications' owners and infrastructure\nmanagers need to be ensured. Despite the rich number of proposals of Machine\nLearning (ML) based management solutions, researchers and practitioners yet\nstruggle to guarantee long-term prediction and control, and accurate\ntroubleshooting. Therefore, we present a novel ML paradigm based on Active\nInference (AIF) -- a concept from neuroscience that describes how the brain\nconstantly predicts and evaluates sensory information to decrease long-term\nsurprise. We implement it and evaluate it in a heterogeneous real stream\nprocessing use case, where an AIF-based agent continuously optimizes the\nfulfillment of three SLOs for three autonomous driving services running on\nmultiple devices. The agent used causal knowledge to gradually develop an\nunderstanding of how its actions are related to requirements fulfillment, and\nwhich configurations to favor. Through this approach, our agent requires up to\nthirty iterations to converge to the optimal solution, showing the capability\nof offering accurate results in a short amount of time. Furthermore, thanks to\nAIF and its causal structures, our method guarantees full transparency on the\ndecision making, making the interpretation of the results and the\ntroubleshooting effortless."}
{"arxiv_id": "2409.17931v1", "published": "2024-09-26T15:08:38Z", "title": "Intelligent Energy Management: Remaining Useful Life Prediction and\n  Charging Automation System Comprised of Deep Learning and the Internet of\n  Things", "summary": "Remaining Useful Life (RUL) of battery is an important parameter to know the\nbattery's remaining life and need for recharge. The goal of this research\nproject is to develop machine learning-based models for the battery RUL\ndataset. Different ML models are developed to classify the RUL of the vehicle,\nand the IoT (Internet of Things) concept is simulated for automating the\ncharging system and managing any faults aligning. The graphs plotted depict the\nrelationship between various vehicle parameters using the Blynk IoT platform.\nResults show that the catboost, Multi-Layer Perceptron (MLP), Gated Recurrent\nUnit (GRU), and hybrid model developed could classify RUL into three classes\nwith 99% more accuracy. The data is fed using the tkinter GUI for simulating\nartificial intelligence (AI)-based charging, and with a pyserial backend, data\ncan be entered into the Esp-32 microcontroller for making charge discharge\npossible with the model's predictions. Also, with an IoT system, the charging\ncan be disconnected, monitored, and analyzed for automation. The results show\nthat an accuracy of 99% can be obtained on models MLP, catboost model and\nsimilar accuracy on GRU model can be obtained, and finally relay-based\ntriggering can be made by prediction through the model used for automating the\ncharging and energy-saving mechanism. By showcasing an exemplary Blynk\nplatform-based monitoring and automation phenomenon, we further present\ninnovative ways of monitoring parameters and automating the system."}
{"arxiv_id": "2409.18127v1", "published": "2024-09-26T17:59:31Z", "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions", "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning."}
{"arxiv_id": "2409.18104v1", "published": "2024-09-26T17:49:20Z", "title": "Find Rhinos without Finding Rhinos: Active Learning with Multimodal\n  Imagery of South African Rhino Habitats", "summary": "Much of Earth's charismatic megafauna is endangered by human activities,\nparticularly the rhino, which is at risk of extinction due to the poaching\ncrisis in Africa. Monitoring rhinos' movement is crucial to their protection\nbut has unfortunately proven difficult because rhinos are elusive. Therefore,\ninstead of tracking rhinos, we propose the novel approach of mapping communal\ndefecation sites, called middens, which give information about rhinos' spatial\nbehavior valuable to anti-poaching, management, and reintroduction efforts.\nThis paper provides the first-ever mapping of rhino midden locations by\nbuilding classifiers to detect them using remotely sensed thermal, RGB, and\nLiDAR imagery in passive and active learning settings. As existing active\nlearning methods perform poorly due to the extreme class imbalance in our\ndataset, we design MultimodAL, an active learning system employing a ranking\ntechnique and multimodality to achieve competitive performance with passive\nlearning models with 94% fewer labels. Our methods could therefore save over 76\nhours in labeling time when used on a similarly-sized dataset. Unexpectedly,\nour midden map reveals that rhino middens are not randomly distributed\nthroughout the landscape; rather, they are clustered. Consequently, rangers\nshould be targeted at areas with high midden densities to strengthen\nanti-poaching efforts, in line with UN Target 15.7."}
{"arxiv_id": "2409.18101v1", "published": "2024-09-26T17:44:52Z", "title": "AI-Powered Augmented Reality for Satellite Assembly, Integration and\n  Test", "summary": "The integration of Artificial Intelligence (AI) and Augmented Reality (AR) is\nset to transform satellite Assembly, Integration, and Testing (AIT) processes\nby enhancing precision, minimizing human error, and improving operational\nefficiency in cleanroom environments. This paper presents a technical\ndescription of the European Space Agency's (ESA) project \"AI for AR in\nSatellite AIT,\" which combines real-time computer vision and AR systems to\nassist technicians during satellite assembly. Leveraging Microsoft HoloLens 2\nas the AR interface, the system delivers context-aware instructions and\nreal-time feedback, tackling the complexities of object recognition and 6D pose\nestimation in AIT workflows. All AI models demonstrated over 70% accuracy, with\nthe detection model exceeding 95% accuracy, indicating a high level of\nperformance and reliability. A key contribution of this work lies in the\neffective use of synthetic data for training AI models in AR applications,\naddressing the significant challenges of obtaining real-world datasets in\nhighly dynamic satellite environments, as well as the creation of the Segmented\nAnything Model for Automatic Labelling (SAMAL), which facilitates the automatic\nannotation of real data, achieving speeds up to 20 times faster than manual\nhuman annotation. The findings demonstrate the efficacy of AI-driven AR systems\nin automating critical satellite assembly tasks, setting a foundation for\nfuture innovations in the space industry."}
{"arxiv_id": "2409.18083v1", "published": "2024-09-26T17:26:18Z", "title": "Stable Video Portraits", "summary": "Rapid advances in the field of generative AI and text-to-image methods in\nparticular have transformed the way we interact with and perceive\ncomputer-generated imagery today. In parallel, much progress has been made in\n3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we\npresent SVP, a novel hybrid 2D/3D generation method that outputs photorealistic\nvideos of talking faces leveraging a large pre-trained text-to-image prior\n(2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific\nfine-tuning of a general 2D stable diffusion model which we lift to a video\nmodel by providing temporal 3DMM sequences as conditioning and by introducing a\ntemporal denoising procedure. As an output, this model generates temporally\nsmooth imagery of a person with 3DMM-based controls, i.e., a person-specific\navatar. The facial appearance of this person-specific avatar can be edited and\nmorphed to text-defined celebrities, without any fine-tuning at test time. The\nmethod is analyzed quantitatively and qualitatively, and we show that our\nmethod outperforms state-of-the-art monocular head avatar methods."}
{"arxiv_id": "2409.18073v1", "published": "2024-09-26T17:19:49Z", "title": "Infer Human's Intentions Before Following Natural Language Instructions", "summary": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat."}
{"arxiv_id": "2409.18052v1", "published": "2024-09-26T16:55:44Z", "title": "Explaining Explaining", "summary": "Explanation is key to people having confidence in high-stakes AI systems.\nHowever, machine-learning-based systems - which account for almost all current\nAI - can't explain because they are usually black boxes. The explainable AI\n(XAI) movement hedges this problem by redefining \"explanation\". The\nhuman-centered explainable AI (HCXAI) movement identifies the\nexplanation-oriented needs of users but can't fulfill them because of its\ncommitment to machine learning. In order to achieve the kinds of explanations\nneeded by real people operating in critical domains, we must rethink how to\napproach AI. We describe a hybrid approach to developing cognitive agents that\nuses a knowledge-based infrastructure supplemented by data obtained through\nmachine learning when applicable. These agents will serve as assistants to\nhumans who will bear ultimate responsibility for the decisions and actions of\nthe human-robot team. We illustrate the explanatory potential of such agents\nusing the under-the-hood panels of a demonstration system in which a team of\nsimulated robots collaborates on a search task assigned by a human."}
{"arxiv_id": "2409.18048v1", "published": "2024-09-26T16:49:57Z", "title": "Next-Gen Software Engineering: AI-Assisted Big Models", "summary": "The effectiveness of model-driven software engineering (MDSE) has been\ndemonstrated in the context of complex software; however, it has not been\nwidely adopted due to the requisite efforts associated with model development\nand maintenance, as well as the specific modelling competencies required for\nMDSE. Concurrently, artificial intelligence (AI) methods, particularly machine\nlearning (ML) methods, have demonstrated considerable abilities when applied to\nthe huge code bases accessible on open-source coding platforms. The so-called\nbig code provides the basis for significant advances in empirical software\nengineering, as well as in the automation of coding processes and improvements\nin software quality with the use of AI. The objective of this paper is to\nfacilitate a synthesis between these two significant domains of software\nengineering (SE), namely models and AI in SE. The paper provides an overview of\nthe current status of AI-assisted software engineering. In light of the\naforementioned considerations, a vision of AI-assisted Big Models in SE is put\nforth, with the aim of capitalising on the advantages inherent to both\napproaches in the context of software development. Finally, the new paradigm of\npair modelling in MDSE is proposed."}
{"arxiv_id": "2409.18039v1", "published": "2024-09-26T16:43:07Z", "title": "Ecosystem-Agnostic Standardization of Quantum Runtime Architecture:\n  Accelerating Utility in Quantum Computing", "summary": "Fault tolerance is a long-term objective driving many companies and research\norganizations to compete in making current, imperfect quantum computers useful\n- Quantum Utility (QU). It looks promising to achieve this by leveraging\nsoftware optimization approaches primarily driven by AI techniques. This\naggressive research covers all layers of Quantum Computing Optimization\nMiddleware (QCOM) and requires execution on real quantum hardware (QH). Due to\nthe nascent nature of the technology domain and the proprietary strategies of\nboth large and small players, popular runtimes for executing quantum workloads\nlack flexibility in programming models, scheduling, and hardware access\npatterns, including queuing, which creates roadblocks for researchers and slows\ninnovation. These problems are further exacerbated by emerging hybrid operating\nmodels that place Graphical Processing Unit (GPU) supercomputing and Quantum\nIntermediate Representation (QIR) at the heart of real-time computations across\nquantum and distributed resources. There is a need for a widely adopted runtime\nplatform (RP) driven by the open-source community that can be easily deployed\nto work in a distributed manner between Quantum Processing Unit (QPU), GPU,\ncontrol hardware, external compute resources and provide required flexibility\nin terms of programming & configuration models."}
{"arxiv_id": "2409.18025v1", "published": "2024-09-26T16:32:19Z", "title": "An Adversarial Perspective on Machine Unlearning for AI Safety", "summary": "Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining."}
{"arxiv_id": "2409.17931v1", "published": "2024-09-26T15:08:38Z", "title": "Intelligent Energy Management: Remaining Useful Life Prediction and\n  Charging Automation System Comprised of Deep Learning and the Internet of\n  Things", "summary": "Remaining Useful Life (RUL) of battery is an important parameter to know the\nbattery's remaining life and need for recharge. The goal of this research\nproject is to develop machine learning-based models for the battery RUL\ndataset. Different ML models are developed to classify the RUL of the vehicle,\nand the IoT (Internet of Things) concept is simulated for automating the\ncharging system and managing any faults aligning. The graphs plotted depict the\nrelationship between various vehicle parameters using the Blynk IoT platform.\nResults show that the catboost, Multi-Layer Perceptron (MLP), Gated Recurrent\nUnit (GRU), and hybrid model developed could classify RUL into three classes\nwith 99% more accuracy. The data is fed using the tkinter GUI for simulating\nartificial intelligence (AI)-based charging, and with a pyserial backend, data\ncan be entered into the Esp-32 microcontroller for making charge discharge\npossible with the model's predictions. Also, with an IoT system, the charging\ncan be disconnected, monitored, and analyzed for automation. The results show\nthat an accuracy of 99% can be obtained on models MLP, catboost model and\nsimilar accuracy on GRU model can be obtained, and finally relay-based\ntriggering can be made by prediction through the model used for automating the\ncharging and energy-saving mechanism. By showcasing an exemplary Blynk\nplatform-based monitoring and automation phenomenon, we further present\ninnovative ways of monitoring parameters and automating the system."}
{"arxiv_id": "2409.18127v1", "published": "2024-09-26T17:59:31Z", "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions", "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning."}
{"arxiv_id": "2409.18104v1", "published": "2024-09-26T17:49:20Z", "title": "Find Rhinos without Finding Rhinos: Active Learning with Multimodal\n  Imagery of South African Rhino Habitats", "summary": "Much of Earth's charismatic megafauna is endangered by human activities,\nparticularly the rhino, which is at risk of extinction due to the poaching\ncrisis in Africa. Monitoring rhinos' movement is crucial to their protection\nbut has unfortunately proven difficult because rhinos are elusive. Therefore,\ninstead of tracking rhinos, we propose the novel approach of mapping communal\ndefecation sites, called middens, which give information about rhinos' spatial\nbehavior valuable to anti-poaching, management, and reintroduction efforts.\nThis paper provides the first-ever mapping of rhino midden locations by\nbuilding classifiers to detect them using remotely sensed thermal, RGB, and\nLiDAR imagery in passive and active learning settings. As existing active\nlearning methods perform poorly due to the extreme class imbalance in our\ndataset, we design MultimodAL, an active learning system employing a ranking\ntechnique and multimodality to achieve competitive performance with passive\nlearning models with 94% fewer labels. Our methods could therefore save over 76\nhours in labeling time when used on a similarly-sized dataset. Unexpectedly,\nour midden map reveals that rhino middens are not randomly distributed\nthroughout the landscape; rather, they are clustered. Consequently, rangers\nshould be targeted at areas with high midden densities to strengthen\nanti-poaching efforts, in line with UN Target 15.7."}
{"arxiv_id": "2409.18101v1", "published": "2024-09-26T17:44:52Z", "title": "AI-Powered Augmented Reality for Satellite Assembly, Integration and\n  Test", "summary": "The integration of Artificial Intelligence (AI) and Augmented Reality (AR) is\nset to transform satellite Assembly, Integration, and Testing (AIT) processes\nby enhancing precision, minimizing human error, and improving operational\nefficiency in cleanroom environments. This paper presents a technical\ndescription of the European Space Agency's (ESA) project \"AI for AR in\nSatellite AIT,\" which combines real-time computer vision and AR systems to\nassist technicians during satellite assembly. Leveraging Microsoft HoloLens 2\nas the AR interface, the system delivers context-aware instructions and\nreal-time feedback, tackling the complexities of object recognition and 6D pose\nestimation in AIT workflows. All AI models demonstrated over 70% accuracy, with\nthe detection model exceeding 95% accuracy, indicating a high level of\nperformance and reliability. A key contribution of this work lies in the\neffective use of synthetic data for training AI models in AR applications,\naddressing the significant challenges of obtaining real-world datasets in\nhighly dynamic satellite environments, as well as the creation of the Segmented\nAnything Model for Automatic Labelling (SAMAL), which facilitates the automatic\nannotation of real data, achieving speeds up to 20 times faster than manual\nhuman annotation. The findings demonstrate the efficacy of AI-driven AR systems\nin automating critical satellite assembly tasks, setting a foundation for\nfuture innovations in the space industry."}
{"arxiv_id": "2409.18083v1", "published": "2024-09-26T17:26:18Z", "title": "Stable Video Portraits", "summary": "Rapid advances in the field of generative AI and text-to-image methods in\nparticular have transformed the way we interact with and perceive\ncomputer-generated imagery today. In parallel, much progress has been made in\n3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we\npresent SVP, a novel hybrid 2D/3D generation method that outputs photorealistic\nvideos of talking faces leveraging a large pre-trained text-to-image prior\n(2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific\nfine-tuning of a general 2D stable diffusion model which we lift to a video\nmodel by providing temporal 3DMM sequences as conditioning and by introducing a\ntemporal denoising procedure. As an output, this model generates temporally\nsmooth imagery of a person with 3DMM-based controls, i.e., a person-specific\navatar. The facial appearance of this person-specific avatar can be edited and\nmorphed to text-defined celebrities, without any fine-tuning at test time. The\nmethod is analyzed quantitatively and qualitatively, and we show that our\nmethod outperforms state-of-the-art monocular head avatar methods."}
{"arxiv_id": "2409.18073v1", "published": "2024-09-26T17:19:49Z", "title": "Infer Human's Intentions Before Following Natural Language Instructions", "summary": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat."}
{"arxiv_id": "2409.18052v1", "published": "2024-09-26T16:55:44Z", "title": "Explaining Explaining", "summary": "Explanation is key to people having confidence in high-stakes AI systems.\nHowever, machine-learning-based systems - which account for almost all current\nAI - can't explain because they are usually black boxes. The explainable AI\n(XAI) movement hedges this problem by redefining \"explanation\". The\nhuman-centered explainable AI (HCXAI) movement identifies the\nexplanation-oriented needs of users but can't fulfill them because of its\ncommitment to machine learning. In order to achieve the kinds of explanations\nneeded by real people operating in critical domains, we must rethink how to\napproach AI. We describe a hybrid approach to developing cognitive agents that\nuses a knowledge-based infrastructure supplemented by data obtained through\nmachine learning when applicable. These agents will serve as assistants to\nhumans who will bear ultimate responsibility for the decisions and actions of\nthe human-robot team. We illustrate the explanatory potential of such agents\nusing the under-the-hood panels of a demonstration system in which a team of\nsimulated robots collaborates on a search task assigned by a human."}
{"arxiv_id": "2409.18048v1", "published": "2024-09-26T16:49:57Z", "title": "Next-Gen Software Engineering: AI-Assisted Big Models", "summary": "The effectiveness of model-driven software engineering (MDSE) has been\ndemonstrated in the context of complex software; however, it has not been\nwidely adopted due to the requisite efforts associated with model development\nand maintenance, as well as the specific modelling competencies required for\nMDSE. Concurrently, artificial intelligence (AI) methods, particularly machine\nlearning (ML) methods, have demonstrated considerable abilities when applied to\nthe huge code bases accessible on open-source coding platforms. The so-called\nbig code provides the basis for significant advances in empirical software\nengineering, as well as in the automation of coding processes and improvements\nin software quality with the use of AI. The objective of this paper is to\nfacilitate a synthesis between these two significant domains of software\nengineering (SE), namely models and AI in SE. The paper provides an overview of\nthe current status of AI-assisted software engineering. In light of the\naforementioned considerations, a vision of AI-assisted Big Models in SE is put\nforth, with the aim of capitalising on the advantages inherent to both\napproaches in the context of software development. Finally, the new paradigm of\npair modelling in MDSE is proposed."}
{"arxiv_id": "2409.18039v1", "published": "2024-09-26T16:43:07Z", "title": "Ecosystem-Agnostic Standardization of Quantum Runtime Architecture:\n  Accelerating Utility in Quantum Computing", "summary": "Fault tolerance is a long-term objective driving many companies and research\norganizations to compete in making current, imperfect quantum computers useful\n- Quantum Utility (QU). It looks promising to achieve this by leveraging\nsoftware optimization approaches primarily driven by AI techniques. This\naggressive research covers all layers of Quantum Computing Optimization\nMiddleware (QCOM) and requires execution on real quantum hardware (QH). Due to\nthe nascent nature of the technology domain and the proprietary strategies of\nboth large and small players, popular runtimes for executing quantum workloads\nlack flexibility in programming models, scheduling, and hardware access\npatterns, including queuing, which creates roadblocks for researchers and slows\ninnovation. These problems are further exacerbated by emerging hybrid operating\nmodels that place Graphical Processing Unit (GPU) supercomputing and Quantum\nIntermediate Representation (QIR) at the heart of real-time computations across\nquantum and distributed resources. There is a need for a widely adopted runtime\nplatform (RP) driven by the open-source community that can be easily deployed\nto work in a distributed manner between Quantum Processing Unit (QPU), GPU,\ncontrol hardware, external compute resources and provide required flexibility\nin terms of programming & configuration models."}
{"arxiv_id": "2409.18025v1", "published": "2024-09-26T16:32:19Z", "title": "An Adversarial Perspective on Machine Unlearning for AI Safety", "summary": "Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining."}
{"arxiv_id": "2409.17931v1", "published": "2024-09-26T15:08:38Z", "title": "Intelligent Energy Management: Remaining Useful Life Prediction and\n  Charging Automation System Comprised of Deep Learning and the Internet of\n  Things", "summary": "Remaining Useful Life (RUL) of battery is an important parameter to know the\nbattery's remaining life and need for recharge. The goal of this research\nproject is to develop machine learning-based models for the battery RUL\ndataset. Different ML models are developed to classify the RUL of the vehicle,\nand the IoT (Internet of Things) concept is simulated for automating the\ncharging system and managing any faults aligning. The graphs plotted depict the\nrelationship between various vehicle parameters using the Blynk IoT platform.\nResults show that the catboost, Multi-Layer Perceptron (MLP), Gated Recurrent\nUnit (GRU), and hybrid model developed could classify RUL into three classes\nwith 99% more accuracy. The data is fed using the tkinter GUI for simulating\nartificial intelligence (AI)-based charging, and with a pyserial backend, data\ncan be entered into the Esp-32 microcontroller for making charge discharge\npossible with the model's predictions. Also, with an IoT system, the charging\ncan be disconnected, monitored, and analyzed for automation. The results show\nthat an accuracy of 99% can be obtained on models MLP, catboost model and\nsimilar accuracy on GRU model can be obtained, and finally relay-based\ntriggering can be made by prediction through the model used for automating the\ncharging and energy-saving mechanism. By showcasing an exemplary Blynk\nplatform-based monitoring and automation phenomenon, we further present\ninnovative ways of monitoring parameters and automating the system."}
{"arxiv_id": "2409.18127v1", "published": "2024-09-26T17:59:31Z", "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions", "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning."}
{"arxiv_id": "2409.18104v1", "published": "2024-09-26T17:49:20Z", "title": "Find Rhinos without Finding Rhinos: Active Learning with Multimodal\n  Imagery of South African Rhino Habitats", "summary": "Much of Earth's charismatic megafauna is endangered by human activities,\nparticularly the rhino, which is at risk of extinction due to the poaching\ncrisis in Africa. Monitoring rhinos' movement is crucial to their protection\nbut has unfortunately proven difficult because rhinos are elusive. Therefore,\ninstead of tracking rhinos, we propose the novel approach of mapping communal\ndefecation sites, called middens, which give information about rhinos' spatial\nbehavior valuable to anti-poaching, management, and reintroduction efforts.\nThis paper provides the first-ever mapping of rhino midden locations by\nbuilding classifiers to detect them using remotely sensed thermal, RGB, and\nLiDAR imagery in passive and active learning settings. As existing active\nlearning methods perform poorly due to the extreme class imbalance in our\ndataset, we design MultimodAL, an active learning system employing a ranking\ntechnique and multimodality to achieve competitive performance with passive\nlearning models with 94% fewer labels. Our methods could therefore save over 76\nhours in labeling time when used on a similarly-sized dataset. Unexpectedly,\nour midden map reveals that rhino middens are not randomly distributed\nthroughout the landscape; rather, they are clustered. Consequently, rangers\nshould be targeted at areas with high midden densities to strengthen\nanti-poaching efforts, in line with UN Target 15.7."}
{"arxiv_id": "2409.18101v1", "published": "2024-09-26T17:44:52Z", "title": "AI-Powered Augmented Reality for Satellite Assembly, Integration and\n  Test", "summary": "The integration of Artificial Intelligence (AI) and Augmented Reality (AR) is\nset to transform satellite Assembly, Integration, and Testing (AIT) processes\nby enhancing precision, minimizing human error, and improving operational\nefficiency in cleanroom environments. This paper presents a technical\ndescription of the European Space Agency's (ESA) project \"AI for AR in\nSatellite AIT,\" which combines real-time computer vision and AR systems to\nassist technicians during satellite assembly. Leveraging Microsoft HoloLens 2\nas the AR interface, the system delivers context-aware instructions and\nreal-time feedback, tackling the complexities of object recognition and 6D pose\nestimation in AIT workflows. All AI models demonstrated over 70% accuracy, with\nthe detection model exceeding 95% accuracy, indicating a high level of\nperformance and reliability. A key contribution of this work lies in the\neffective use of synthetic data for training AI models in AR applications,\naddressing the significant challenges of obtaining real-world datasets in\nhighly dynamic satellite environments, as well as the creation of the Segmented\nAnything Model for Automatic Labelling (SAMAL), which facilitates the automatic\nannotation of real data, achieving speeds up to 20 times faster than manual\nhuman annotation. The findings demonstrate the efficacy of AI-driven AR systems\nin automating critical satellite assembly tasks, setting a foundation for\nfuture innovations in the space industry."}
{"arxiv_id": "2409.18083v1", "published": "2024-09-26T17:26:18Z", "title": "Stable Video Portraits", "summary": "Rapid advances in the field of generative AI and text-to-image methods in\nparticular have transformed the way we interact with and perceive\ncomputer-generated imagery today. In parallel, much progress has been made in\n3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we\npresent SVP, a novel hybrid 2D/3D generation method that outputs photorealistic\nvideos of talking faces leveraging a large pre-trained text-to-image prior\n(2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific\nfine-tuning of a general 2D stable diffusion model which we lift to a video\nmodel by providing temporal 3DMM sequences as conditioning and by introducing a\ntemporal denoising procedure. As an output, this model generates temporally\nsmooth imagery of a person with 3DMM-based controls, i.e., a person-specific\navatar. The facial appearance of this person-specific avatar can be edited and\nmorphed to text-defined celebrities, without any fine-tuning at test time. The\nmethod is analyzed quantitatively and qualitatively, and we show that our\nmethod outperforms state-of-the-art monocular head avatar methods."}
{"arxiv_id": "2409.18073v1", "published": "2024-09-26T17:19:49Z", "title": "Infer Human's Intentions Before Following Natural Language Instructions", "summary": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat."}
{"arxiv_id": "2409.18052v1", "published": "2024-09-26T16:55:44Z", "title": "Explaining Explaining", "summary": "Explanation is key to people having confidence in high-stakes AI systems.\nHowever, machine-learning-based systems - which account for almost all current\nAI - can't explain because they are usually black boxes. The explainable AI\n(XAI) movement hedges this problem by redefining \"explanation\". The\nhuman-centered explainable AI (HCXAI) movement identifies the\nexplanation-oriented needs of users but can't fulfill them because of its\ncommitment to machine learning. In order to achieve the kinds of explanations\nneeded by real people operating in critical domains, we must rethink how to\napproach AI. We describe a hybrid approach to developing cognitive agents that\nuses a knowledge-based infrastructure supplemented by data obtained through\nmachine learning when applicable. These agents will serve as assistants to\nhumans who will bear ultimate responsibility for the decisions and actions of\nthe human-robot team. We illustrate the explanatory potential of such agents\nusing the under-the-hood panels of a demonstration system in which a team of\nsimulated robots collaborates on a search task assigned by a human."}
{"arxiv_id": "2409.18048v1", "published": "2024-09-26T16:49:57Z", "title": "Next-Gen Software Engineering: AI-Assisted Big Models", "summary": "The effectiveness of model-driven software engineering (MDSE) has been\ndemonstrated in the context of complex software; however, it has not been\nwidely adopted due to the requisite efforts associated with model development\nand maintenance, as well as the specific modelling competencies required for\nMDSE. Concurrently, artificial intelligence (AI) methods, particularly machine\nlearning (ML) methods, have demonstrated considerable abilities when applied to\nthe huge code bases accessible on open-source coding platforms. The so-called\nbig code provides the basis for significant advances in empirical software\nengineering, as well as in the automation of coding processes and improvements\nin software quality with the use of AI. The objective of this paper is to\nfacilitate a synthesis between these two significant domains of software\nengineering (SE), namely models and AI in SE. The paper provides an overview of\nthe current status of AI-assisted software engineering. In light of the\naforementioned considerations, a vision of AI-assisted Big Models in SE is put\nforth, with the aim of capitalising on the advantages inherent to both\napproaches in the context of software development. Finally, the new paradigm of\npair modelling in MDSE is proposed."}
{"arxiv_id": "2409.18039v1", "published": "2024-09-26T16:43:07Z", "title": "Ecosystem-Agnostic Standardization of Quantum Runtime Architecture:\n  Accelerating Utility in Quantum Computing", "summary": "Fault tolerance is a long-term objective driving many companies and research\norganizations to compete in making current, imperfect quantum computers useful\n- Quantum Utility (QU). It looks promising to achieve this by leveraging\nsoftware optimization approaches primarily driven by AI techniques. This\naggressive research covers all layers of Quantum Computing Optimization\nMiddleware (QCOM) and requires execution on real quantum hardware (QH). Due to\nthe nascent nature of the technology domain and the proprietary strategies of\nboth large and small players, popular runtimes for executing quantum workloads\nlack flexibility in programming models, scheduling, and hardware access\npatterns, including queuing, which creates roadblocks for researchers and slows\ninnovation. These problems are further exacerbated by emerging hybrid operating\nmodels that place Graphical Processing Unit (GPU) supercomputing and Quantum\nIntermediate Representation (QIR) at the heart of real-time computations across\nquantum and distributed resources. There is a need for a widely adopted runtime\nplatform (RP) driven by the open-source community that can be easily deployed\nto work in a distributed manner between Quantum Processing Unit (QPU), GPU,\ncontrol hardware, external compute resources and provide required flexibility\nin terms of programming & configuration models."}
{"arxiv_id": "2409.18025v1", "published": "2024-09-26T16:32:19Z", "title": "An Adversarial Perspective on Machine Unlearning for AI Safety", "summary": "Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining."}
{"arxiv_id": "2409.17931v1", "published": "2024-09-26T15:08:38Z", "title": "Intelligent Energy Management: Remaining Useful Life Prediction and\n  Charging Automation System Comprised of Deep Learning and the Internet of\n  Things", "summary": "Remaining Useful Life (RUL) of battery is an important parameter to know the\nbattery's remaining life and need for recharge. The goal of this research\nproject is to develop machine learning-based models for the battery RUL\ndataset. Different ML models are developed to classify the RUL of the vehicle,\nand the IoT (Internet of Things) concept is simulated for automating the\ncharging system and managing any faults aligning. The graphs plotted depict the\nrelationship between various vehicle parameters using the Blynk IoT platform.\nResults show that the catboost, Multi-Layer Perceptron (MLP), Gated Recurrent\nUnit (GRU), and hybrid model developed could classify RUL into three classes\nwith 99% more accuracy. The data is fed using the tkinter GUI for simulating\nartificial intelligence (AI)-based charging, and with a pyserial backend, data\ncan be entered into the Esp-32 microcontroller for making charge discharge\npossible with the model's predictions. Also, with an IoT system, the charging\ncan be disconnected, monitored, and analyzed for automation. The results show\nthat an accuracy of 99% can be obtained on models MLP, catboost model and\nsimilar accuracy on GRU model can be obtained, and finally relay-based\ntriggering can be made by prediction through the model used for automating the\ncharging and energy-saving mechanism. By showcasing an exemplary Blynk\nplatform-based monitoring and automation phenomenon, we further present\ninnovative ways of monitoring parameters and automating the system."}
{"arxiv_id": "2409.18131v1", "published": "2024-09-26T17:59:57Z", "title": "Two-dopant origin of competing stripe and pair formation in Hubbard and\n  $t$-$J$ models", "summary": "Understanding the physics of the two-dimensional Hubbard model is widely\nbelieved to be a key step in achieving a full understanding of\nhigh-$T_\\mathrm{c}$ cuprate superconductors. In recent years, progress has been\nmade by large-scale numerical simulations at finite doping and, on the other\nhand, by microscopic theories able to capture the physics of individual charge\ncarriers. In this work, we study single pairs of dopants in a cylindrical\nsystem using the density-matrix renormalization group algorithm. We identify\ntwo coexisting charge configurations that couple to the spin environment in\ndifferent ways: A tightly bound configuration featuring (next-)nearest-neighbor\npairs and a stripe-like configuration of dopants on opposite sides of the\ncylinder, accompanied by a spin domain wall. Thus, we establish that the\ninterplay between stripe order and uniform pairing, central to the models'\nphases at finite doping, has its origin at the single-pair level. By\ninterpolating between the Hubbard and the related $t$-$J$ model, we are able to\nquantitatively understand discrepancies in the pairing properties of the two\nmodels through the three-site hopping term usually omitted from the $t$-$J$\nHamiltonian. This term is closely related to a next-nearest-neighbor tunneling\n$t'$, which we observe to upset the balance between the competing stripe and\npair states on the two-dopant level."}
{"arxiv_id": "2409.18129v1", "published": "2024-09-26T17:59:55Z", "title": "TOI-5005 b: A super-Neptune in the savanna near the ridge", "summary": "The Neptunian desert and savanna have been recently found to be separated by\na ridge, an overdensity of planets in the $\\simeq$3-5 days period range. These\nfeatures are thought to be shaped by dynamical and atmospheric processes.\nHowever, their relative roles are not yet well understood. We intend to confirm\nand characterise the super-Neptune TESS candidate TOI-5005.01, which orbits a\nmoderately bright (V = 11.8) solar-type star (G2 V) with an orbital period of\n6.3 days. We confirm TOI-5005 b to be a transiting super-Neptune with a radius\nof $R_{\\rm p}$ = $6.25\\pm 0.24$ $\\rm R_{\\rm \\oplus}$ ($R_{\\rm p}$ = $0.558\\pm\n0.021$ $\\rm R_{\\rm J}$) and a mass of $M_{\\rm p}$ = $32.7\\pm 5.9$ $\\rm\nM_{\\oplus}$ ($M_{\\rm p}$ = $0.103\\pm 0.018$ $\\rm M_{\\rm J}$), which corresponds\nto a mean density of $\\rho_{\\rm p}$ = $0.74 \\pm 0.16$ $\\rm g \\, cm^{-3}$. Our\ninternal structure modelling indicates that the overall metal mass fraction is\nwell constrained to a value slightly lower than that of Neptune and Uranus\n($Z_{\\rm planet}$ = $0.76^{+0.04}_{-0.11}$). We also estimated the present-day\natmospheric mass-loss rate of TOI-5005 b but found contrasting predictions\ndepending on the choice of photoevaporation model. At a population level, we\nfind statistical evidence ($p$-value = $0.0092^{+0.0184}_{-0.0066}$) that\nplanets in the savanna such as TOI-5005 b tend to show lower densities than\nplanets in the ridge, with a dividing line around 1 $\\rm g \\, cm^{-3}$, which\nsupports the hypothesis of different evolutionary pathways populating both\nregimes. TOI-5005 b is located in a key region of the period-radius space to\nstudy the transition between the Neptunian ridge and the savanna. It orbits the\nbrightest star of all such planets, which makes it a target of interest for\natmospheric and orbital architecture observations that will bring a clearer\npicture of its overall evolution."}
{"arxiv_id": "2409.18130v1", "published": "2024-09-26T17:59:55Z", "title": "Bridging 4D QFTs and 2D VOAs via 3D high-temperature EFTs", "summary": "The high-temperature limit of the superconformal index, especially on higher\nsheets, often captures useful universal information about a theory. In 4d\n$\\mathcal{N}=2$ superconformal field theories with fractional r-charges, there\nexists a special notion of high-temperature limit on higher sheets that\ncaptures data of three-dimensional topological quantum field theories arising\nfrom r-twisted circle reduction. These TQFTs are closely tied with the VOA of\nthe 4d SCFT. We study such high-temperature limits. More specifically, we apply\nDi~Pietro-Komargodski type supersymmetric effective field theory techniques to\nr-twisted circle reductions of $(A_1,A_{2n})$ Argyres-Douglas theories,\nleveraging their Maruyoshi-Song Lagrangian with manifest $\\mathcal{N}=1$\nsupersymmetry. The result on the second sheet is the Gang-Kim-Stubbs family of\n3d $\\mathcal{N}=2$ SUSY enhancing rank-$0$ theories with monopole\nsuperpotentials, whose boundary supports the Virasoro minimal model VOAs\n$M(2,2n+3)$. Upon topological twist, they give non-unitary TQFTs controlled by\nthe $M(2,2n+3)$ modular tensor category (MTC). The high-temperature limit on\nother sheets yields their unitary or non-unitary Galois conjugates. This opens\nup the prospect of a broader four-supercharge perspective on the celebrated\ncorrespondence between 4d $\\mathcal{N}=2$ SCFTs and 2d VOAs via interpolating\n3d EFTs. Several byproducts follow, including a systematic approach to 3d SUSY\nenhancement from 4d SUSY enhancement, and a 3d QFT handle on Galois orbits of\nvarious MTCs associated with 4d $\\mathcal{N}=2$ SCFTs."}
{"arxiv_id": "2409.18128v1", "published": "2024-09-26T17:59:51Z", "title": "FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity\n  Refiner", "summary": "Building on the success of diffusion models in visual generation, flow-based\nmodels reemerge as another prominent family of generative models that have\nachieved competitive or better performance in terms of both visual quality and\ninference speed. By learning the velocity field through flow-matching,\nflow-based models tend to produce a straighter sampling trajectory, which is\nadvantageous during the sampling process. However, unlike diffusion models for\nwhich fast samplers are well-developed, efficient sampling of flow-based\ngenerative models has been rarely explored. In this paper, we propose a\nframework called FlowTurbo to accelerate the sampling of flow-based models\nwhile still enhancing the sampling quality. Our primary observation is that the\nvelocity predictor's outputs in the flow-based models will become stable during\nthe sampling, enabling the estimation of velocity via a lightweight velocity\nrefiner. Additionally, we introduce several techniques including a pseudo\ncorrector and sample-aware compilation to further reduce inference time. Since\nFlowTurbo does not change the multi-step sampling paradigm, it can be\neffectively applied for various tasks such as image editing, inpainting, etc.\nBy integrating FlowTurbo into different flow-based models, we obtain an\nacceleration ratio of 53.1%$\\sim$58.3% on class-conditional generation and\n29.8%$\\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID\nof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),\nachieving the real-time image generation and establishing the new\nstate-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo."}
{"arxiv_id": "2409.18127v1", "published": "2024-09-26T17:59:31Z", "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions", "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning."}
{"arxiv_id": "2409.18126v1", "published": "2024-09-26T17:59:12Z", "title": "Boltzmann Sampling by Diabatic Quantum Annealing", "summary": "It has been proposed that diabatic quantum annealing (DQA), which turns off\nthe transverse field at a finite speed, produces samples well described by the\nBoltzmann distribution. We analytically show that, up to linear order in\nquenching time, the DQA approximates a high-temperature Boltzmann distribution.\nOur theory yields an explicit relation between the quenching rate of the DQA\nand the temperature of the Boltzmann distribution. Based on this result, we\ndiscuss how the DQA can be utilized to train the Restricted Boltzmann Machine\n(RBM)."}
{"arxiv_id": "2409.18125v1", "published": "2024-09-26T17:59:11Z", "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with\n  3D-awareness", "summary": "Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced\ntheir proficiency in 2D visual understanding tasks, enabling them to\neffectively process and understand images and videos. However, the development\nof LMMs with 3D-awareness for 3D scene understanding has been hindered by the\nlack of large-scale 3D vision-language datasets and powerful 3D encoders. In\nthis paper, we introduce a simple yet effective framework called LLaVA-3D.\nLeveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D\nefficiently adapts LLaVA for 3D scene understanding without compromising 2D\nunderstanding capabilities. To achieve this, we employ a simple yet effective\nrepresentation, 3D Patch, which connects 2D CLIP patch features with their\ncorresponding positions in 3D space. By integrating the 3D Patches into 2D LMMs\nand employing joint 2D and 3D vision-language instruction tuning, we establish\na unified architecture for both 2D image understanding and 3D scene\nunderstanding. Experimental results show that LLaVA-3D converges 3.5x faster\nthan existing 3D LMMs when trained on 3D vision-language datasets. Moreover,\nLLaVA-3D not only achieves state-of-the-art performance across various 3D tasks\nbut also maintains comparable 2D image understanding and vision-language\nconversation capabilities with LLaVA."}
{"arxiv_id": "2409.18124v1", "published": "2024-09-26T17:58:55Z", "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense\n  Prediction", "summary": "Leveraging the visual priors of pre-trained text-to-image diffusion models\noffers a promising solution to enhance zero-shot generalization in dense\nprediction tasks. However, existing methods often uncritically use the original\ndiffusion formulation, which may not be optimal due to the fundamental\ndifferences between dense prediction and image generation. In this paper, we\nprovide a systemic analysis of the diffusion formulation for the dense\nprediction, focusing on both quality and efficiency. And we find that the\noriginal parameterization type for image generation, which learns to predict\nnoise, is harmful for dense prediction; the multi-step noising/denoising\ndiffusion process is also unnecessary and challenging to optimize. Based on\nthese insights, we introduce Lotus, a diffusion-based visual foundation model\nwith a simple yet effective adaptation protocol for dense prediction.\nSpecifically, Lotus is trained to directly predict annotations instead of\nnoise, thereby avoiding harmful variance. We also reformulate the diffusion\nprocess into a single-step procedure, simplifying optimization and\nsignificantly boosting inference speed. Additionally, we introduce a novel\ntuning strategy called detail preserver, which achieves more accurate and\nfine-grained predictions. Without scaling up the training data or model\ncapacity, Lotus achieves SoTA performance in zero-shot depth and normal\nestimation across various datasets. It also significantly enhances efficiency,\nbeing hundreds of times faster than most existing diffusion-based methods."}
{"arxiv_id": "2409.18122v1", "published": "2024-09-26T17:58:05Z", "title": "RT-GuIDE: Real-Time Gaussian splatting for Information-Driven\n  Exploration", "summary": "We propose a framework for active mapping and exploration that leverages\nGaussian splatting for constructing information-rich maps. Further, we develop\na parallelized motion planning algorithm that can exploit the Gaussian map for\nreal-time navigation. The Gaussian map constructed onboard the robot is\noptimized for both photometric and geometric quality while enabling real-time\nsituational awareness for autonomy. We show through simulation experiments that\nour method is competitive with approaches that use alternate information gain\nmetrics, while being orders of magnitude faster to compute. In real-world\nexperiments, our algorithm achieves better map quality (10% higher Peak\nSignal-to-Noise Ratio (PSNR) and 30% higher geometric reconstruction accuracy)\nthan Gaussian maps constructed by traditional exploration baselines. Experiment\nvideos and more details can be found on our project page:\nhttps://tyuezhan.github.io/RT_GuIDE/"}
{"arxiv_id": "2409.18121v1", "published": "2024-09-26T17:57:16Z", "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with\n  Monocular 4D Reconstruction", "summary": "Humans can learn to manipulate new objects by simply watching others;\nproviding robots with the ability to learn from such demonstrations would\nenable a natural interface specifying new behaviors. This work develops Robot\nSee Robot Do (RSRD), a method for imitating articulated object manipulation\nfrom a single monocular RGB human demonstration given a single static\nmulti-view object scan. We first propose 4D Differentiable Part Models\n(4D-DPM), a method for recovering 3D part motion from a monocular video with\ndifferentiable rendering. This analysis-by-synthesis approach uses part-centric\nfeature fields in an iterative optimization which enables the use of geometric\nregularizers to recover 3D motions from only a single video. Given this 4D\nreconstruction, the robot replicates object trajectories by planning bimanual\narm motions that induce the demonstrated object part motion. By representing\ndemonstrations as part-centric trajectories, RSRD focuses on replicating the\ndemonstration's intended behavior while considering the robot's own\nmorphological limits, rather than attempting to reproduce the hand's motion. We\nevaluate 4D-DPM's 3D tracking accuracy on ground truth annotated 3D part\ntrajectories and RSRD's physical execution performance on 9 objects across 10\ntrials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of\n87% success rate, for a total end-to-end success rate of 60% across 90 trials.\nNotably, this is accomplished using only feature fields distilled from large\npretrained vision models -- without any task-specific training, fine-tuning,\ndataset collection, or annotation. Project page:\nhttps://robot-see-robot-do.github.io"}
{"arxiv_id": "2409.18091v1", "published": "2024-09-26T17:38:33Z", "title": "Incorporating sparse labels into biologging studies using hidden Markov\n  models with weighted likelihoods", "summary": "Ecologists often use a hidden Markov model to decode a latent process, such\nas a sequence of an animal's behaviours, from an observed biologging time\nseries. Modern technological devices such as video recorders and drones now\nallow researchers to directly observe an animal's behaviour. Using these\nobservations as labels of the latent process can improve a hidden Markov\nmodel's accuracy when decoding the latent process. However, many wild animals\nare observed infrequently. Including such rare labels often has a negligible\ninfluence on parameter estimates, which in turn does not meaningfully improve\nthe accuracy of the decoded latent process. We introduce a weighted likelihood\napproach that increases the relative influence of labelled observations. We use\nthis approach to develop two hidden Markov models to decode the foraging\nbehaviour of killer whales (Orcinus orca) off the coast of British Columbia,\nCanada. Using cross-validated evaluation metrics, we show that our weighted\nlikelihood approach produces more accurate and understandable decoded latent\nprocesses compared to existing methods. Thus, our method effectively leverages\nsparse labels to enhance researchers' ability to accurately decode hidden\nprocesses across various fields."}
{"arxiv_id": "2409.18048v1", "published": "2024-09-26T16:49:57Z", "title": "Next-Gen Software Engineering: AI-Assisted Big Models", "summary": "The effectiveness of model-driven software engineering (MDSE) has been\ndemonstrated in the context of complex software; however, it has not been\nwidely adopted due to the requisite efforts associated with model development\nand maintenance, as well as the specific modelling competencies required for\nMDSE. Concurrently, artificial intelligence (AI) methods, particularly machine\nlearning (ML) methods, have demonstrated considerable abilities when applied to\nthe huge code bases accessible on open-source coding platforms. The so-called\nbig code provides the basis for significant advances in empirical software\nengineering, as well as in the automation of coding processes and improvements\nin software quality with the use of AI. The objective of this paper is to\nfacilitate a synthesis between these two significant domains of software\nengineering (SE), namely models and AI in SE. The paper provides an overview of\nthe current status of AI-assisted software engineering. In light of the\naforementioned considerations, a vision of AI-assisted Big Models in SE is put\nforth, with the aim of capitalising on the advantages inherent to both\napproaches in the context of software development. Finally, the new paradigm of\npair modelling in MDSE is proposed."}
{"arxiv_id": "2409.18039v1", "published": "2024-09-26T16:43:07Z", "title": "Ecosystem-Agnostic Standardization of Quantum Runtime Architecture:\n  Accelerating Utility in Quantum Computing", "summary": "Fault tolerance is a long-term objective driving many companies and research\norganizations to compete in making current, imperfect quantum computers useful\n- Quantum Utility (QU). It looks promising to achieve this by leveraging\nsoftware optimization approaches primarily driven by AI techniques. This\naggressive research covers all layers of Quantum Computing Optimization\nMiddleware (QCOM) and requires execution on real quantum hardware (QH). Due to\nthe nascent nature of the technology domain and the proprietary strategies of\nboth large and small players, popular runtimes for executing quantum workloads\nlack flexibility in programming models, scheduling, and hardware access\npatterns, including queuing, which creates roadblocks for researchers and slows\ninnovation. These problems are further exacerbated by emerging hybrid operating\nmodels that place Graphical Processing Unit (GPU) supercomputing and Quantum\nIntermediate Representation (QIR) at the heart of real-time computations across\nquantum and distributed resources. There is a need for a widely adopted runtime\nplatform (RP) driven by the open-source community that can be easily deployed\nto work in a distributed manner between Quantum Processing Unit (QPU), GPU,\ncontrol hardware, external compute resources and provide required flexibility\nin terms of programming & configuration models."}
{"arxiv_id": "2409.18020v1", "published": "2024-09-26T16:28:04Z", "title": "Exploring the Dynamics of CME-Driven Shocks by Comparing Numerical\n  Modeling and Observations", "summary": "Shocks driven by coronal mass ejections (CMEs) are primary drivers of gradual\nsolar energetic particle (SEP) events, posing significant risks to space\ntechnology and astronauts. Concurrently, particles accelerated at these shocks\nmay also propagate back to the Sun, potentially generating gamma-ray emissions\nthrough pion decay. We incorporated advanced modeling and multi-messenger\nobservations to explore the role of CME-driven shocks in gamma-ray emissions\nand SEPs. Motivated by Fermi-LAT long-duration solar flares, we used the AWSoM\nMHD model to investigate the connection between the shocks and the properties\nof observed gamma-ray emissions. By coupling the AWSoM with iPATH model, we\nevaluate the impact of shock evolution complexity near the Sun on SEP intensity\nand spectra. Our result points to the importance of accurate background coronal\nand solar wind modeling, as well as detailed observations of CME source\nregions, in advancing our understanding of CME-driven shocks and the dynamics\nof associated energetic particles."}
{"arxiv_id": "2409.18011v1", "published": "2024-09-26T16:19:56Z", "title": "Entropy-based feature selection for capturing impacts in Earth system\n  models with extreme forcing", "summary": "This paper presents the development of a new entropy-based feature selection\nmethod for identifying and quantifying impacts. Here, impacts are defined as\nstatistically significant differences in spatio-temporal fields when comparing\ndatasets with and without an external forcing in Earth system models. Temporal\nfeature selection is performed by first computing the cross-fuzzy entropy to\nquantify similarity of patterns between two datasets and then applying\nchangepoint detection to identify regions of statistically constant entropy.\nThe method is used to capture temperate north surface cooling from a 9-member\nsimulation ensemble of the Mt. Pinatubo volcanic eruption, which injected 10 Tg\nof SO2 into the stratosphere. The results estimate a mean difference decrease\nin near surface air temperature of -0.560 K with a 99% confidence interval\nbetween -0.864 K and -0.257 K between April and November of 1992, one year\nfollowing the eruption. A sensitivity analysis with decreasing SO2 injection\nrevealed that the impact is statistically significant at 5 Tg but not at 3 Tg.\nUsing identified features, a dependency graph model composed of 68 nodes and\n229 edges directly connecting initial aerosol optical depth changes in the\ntropics to solar flux and temperature changes before the temperate north\nsurface cooling is presented."}
{"arxiv_id": "2409.17980v1", "published": "2024-09-26T15:53:14Z", "title": "Formal verification of higher dimensional quantum protocols", "summary": "Formal methods have been a successful approach for modelling and verifying\nthe correctness of complex technologies like microprocessor chip design,\nbiological systems and others. This is the main motivation of developing\nquantum formal techniques which is to describe and analyse quantum information\nprocessing systems. Our previous work demonstrates the possibility of using a\nquantum process calculus called Communicating Quantum Processes (CQP) to model\nand describe higher dimensional quantum systems. By developing the theory to\ngeneralise the fundamental gates and Bell states, we have modelled quantum\nqudit protocols like teleportation and superdense coding in CQP. In this paper,\nwe demonstrate the use of CQP to analyse higher dimensional quantum protocols.\nThe main idea is to define two processes, one modelling the real protocol and\nthe other expressing a specification, and prove that they are behaviourally\nequivalent. This is a work-in-progress and we present our preliminary results\nin extending the theory of behavioural equivalence in CQP to verify higher\ndimensional quantum protocols using qudits."}
{"arxiv_id": "2409.17973v1", "published": "2024-09-26T15:47:51Z", "title": "Manufacturing, processing, applications, and advancements of Fe-based\n  shape memory alloys", "summary": "Fe-based shape memory alloys (Fe-SMAs) belong to smart metallic materials\nthat can memorize or restore their preset shape after experiencing a\nsubstantial amount of deformation under heat, stress, or magnetic stimuli.\nFe-SMAs have remarkable thermomechanical properties and have attracted\nsignificant interest because of their potential merits, such as cost-effective\nalloying elements, superior workability, weldability, a stable superelastic\nresponse, and low-temperature dependence of critical stress required for\nstress-induced martensitic transformation. Therefore, Fe-SMAs can be an\nintriguing and economical alternative to other SMAs. The recent advancements in\nfabrication methods of conventional metals and SMAs are helping the production\nof customized powder composition and then customized geometries by additive\nmanufacturing (AM). The technology in these areas, i.e., fabrication\ntechniques, experimental characterization, and theoretical formulations of\nFe-SMAs for conventional and AM has been rapidly advancing and is lacking a\ncomprehensive review. This paper provides a critical review of the recent\ndevelopments in Fe-SMAs-related research. The conventional and AM-based methods\nof producing Fe-SMAs are discussed, and a detailed review of the current\nresearch trends on Fe-SMAs including 4-D printing of Fe-SMAs are\ncomprehensively documented. The presented review provides a comprehensive\nreview of experimental methods and processes used to determine the material\ncharacteristics and features of Fe-SMAs. In addition, the work provides a\nreview of the reported computational modeling of Fe-SMAs to help design new\nFe-SMA composition and geometry. Finally, different Fe-SMAs-based applications\nsuch as sensing and damping systems, tube coupling, and reinforced concrete are\nalso discussed."}
{"arxiv_id": "2409.17959v1", "published": "2024-09-26T15:36:14Z", "title": "A Policy Report Evaluating the National Assessment Program for Literacy\n  and Numeracy (Naplan) Reform in Australia: The Impacts of High Stakes\n  Assessment on Students", "summary": "The National Assessment Program for Literacy and Numeracy (NAPLAN) Reform in\nAustralia, launched in 2008, has emerged as the country's most significant and\ncontentious reform. However, due to its high-stakes nature and standardization,\ntesting presents various challenges. These challenges include the combination\nof accountability with the 'My School' website, overlooking higher-order\ncognitive abilities, exacerbating students' anxiety and stress, and creating\ninequity for Language Background Other Than English (LBOTE) students. This\nreport assesses the achievements and obstacles of the NAPLAN reform, proposing\nrecommendations such as transitioning to online testing, enhancing content and\nplatforms, increasing public assessment literacy, and investing more in LBOTE\neducation. These suggestions aim to strike a balance between standardized\ntesting and authentic educational pursuits, adapting to the evolving needs of\nstudents to create a fair, inclusive educational environment that addresses the\ndemands of the 21st century."}
{"arxiv_id": "2409.17945v1", "published": "2024-09-26T15:20:21Z", "title": "Modular Autonomous Vehicle in Heterogeneous Traffic Flow: Modeling,\n  Simulation, and Implication", "summary": "Modular autonomous vehicles (MAVs) represent a groundbreaking concept that\nintegrates modularity into the ongoing development of autonomous vehicles. This\ninnovative design introduces unique features to traffic flow, allowing multiple\nmodules to seamlessly join together and operate collectively. To understand the\ntraffic flow characteristics involving these vehicles and their collective\noperations, this study established a modeling framework specifically designed\nto simulate their behavior within traffic flow. The mixed traffic flow,\nincorporating arbitrarily formed trains of various modular sizes, is modeled\nand studied. Simulations are conducted under varying levels of traffic demand\nand penetration rates to examine the traffic flow dynamics in the presence of\nthese vehicles and their operations. The microscopic trajectories, MAV train\ncompositions, and macroscopic fundamental diagrams of the mixed traffic flow\nare analyzed. The simulation findings indicate that integrating MAVs and their\ncollective operations can substantially enhance capacity, with the extent of\nimprovement depending on the penetration rate in mixed traffic flow. Notably,\nthe capacity nearly doubles when the penetration rate exceeds 75%. Furthermore,\ntheir presence significantly influences and regulates the free-flow speed of\nthe mixed traffic. Particularly, when variations in operational speed limits\nexist between the MAVs and the background traffic, the mixed traffic adjusts to\nthe operating velocity of these vehicles. This study provides insights into\npotential future traffic flow systems incorporating emerging MAV technologies."}
{"arxiv_id": "2409.17916v1", "published": "2024-09-26T15:02:11Z", "title": "Observer-Based Discontinuous Communication in the Secondary Control of\n  AC Microgrids", "summary": "This paper proposes an observer-based event-driven approach to decrease the\noveruse of communication networks. The suggested approach aims to estimate the\nrequired data for sharing between units in line with as much communication\nreduction as possible. In other words, the proposed approach effectively\ndetermines which state variables should be shared (observer concept) among the\nunits during specific time intervals (event-triggered concept). This strategy\nsignificantly reduces the overall communication load. It is shown that the\nestimation error remains bounded and Zeno behavior, characterized by an endless\nnumber of transmissions occurring within a limited time frame, does not occur.\nThe proposed methodology can be systematically applied to any\ncommunication-based secondary controller in alternating current (AC)\nmicrogrids. Simulation results demonstrate a high degree of precision in\nestimating the states under the proposed approach. Also, the secondary\ncontroller performance under the proposed method is evaluated in\nMATLAB/Simulink environment."}
{"arxiv_id": "2409.18091v1", "published": "2024-09-26T17:38:33Z", "title": "Incorporating sparse labels into biologging studies using hidden Markov\n  models with weighted likelihoods", "summary": "Ecologists often use a hidden Markov model to decode a latent process, such\nas a sequence of an animal's behaviours, from an observed biologging time\nseries. Modern technological devices such as video recorders and drones now\nallow researchers to directly observe an animal's behaviour. Using these\nobservations as labels of the latent process can improve a hidden Markov\nmodel's accuracy when decoding the latent process. However, many wild animals\nare observed infrequently. Including such rare labels often has a negligible\ninfluence on parameter estimates, which in turn does not meaningfully improve\nthe accuracy of the decoded latent process. We introduce a weighted likelihood\napproach that increases the relative influence of labelled observations. We use\nthis approach to develop two hidden Markov models to decode the foraging\nbehaviour of killer whales (Orcinus orca) off the coast of British Columbia,\nCanada. Using cross-validated evaluation metrics, we show that our weighted\nlikelihood approach produces more accurate and understandable decoded latent\nprocesses compared to existing methods. Thus, our method effectively leverages\nsparse labels to enhance researchers' ability to accurately decode hidden\nprocesses across various fields."}
{"arxiv_id": "2409.18048v1", "published": "2024-09-26T16:49:57Z", "title": "Next-Gen Software Engineering: AI-Assisted Big Models", "summary": "The effectiveness of model-driven software engineering (MDSE) has been\ndemonstrated in the context of complex software; however, it has not been\nwidely adopted due to the requisite efforts associated with model development\nand maintenance, as well as the specific modelling competencies required for\nMDSE. Concurrently, artificial intelligence (AI) methods, particularly machine\nlearning (ML) methods, have demonstrated considerable abilities when applied to\nthe huge code bases accessible on open-source coding platforms. The so-called\nbig code provides the basis for significant advances in empirical software\nengineering, as well as in the automation of coding processes and improvements\nin software quality with the use of AI. The objective of this paper is to\nfacilitate a synthesis between these two significant domains of software\nengineering (SE), namely models and AI in SE. The paper provides an overview of\nthe current status of AI-assisted software engineering. In light of the\naforementioned considerations, a vision of AI-assisted Big Models in SE is put\nforth, with the aim of capitalising on the advantages inherent to both\napproaches in the context of software development. Finally, the new paradigm of\npair modelling in MDSE is proposed."}
{"arxiv_id": "2409.18039v1", "published": "2024-09-26T16:43:07Z", "title": "Ecosystem-Agnostic Standardization of Quantum Runtime Architecture:\n  Accelerating Utility in Quantum Computing", "summary": "Fault tolerance is a long-term objective driving many companies and research\norganizations to compete in making current, imperfect quantum computers useful\n- Quantum Utility (QU). It looks promising to achieve this by leveraging\nsoftware optimization approaches primarily driven by AI techniques. This\naggressive research covers all layers of Quantum Computing Optimization\nMiddleware (QCOM) and requires execution on real quantum hardware (QH). Due to\nthe nascent nature of the technology domain and the proprietary strategies of\nboth large and small players, popular runtimes for executing quantum workloads\nlack flexibility in programming models, scheduling, and hardware access\npatterns, including queuing, which creates roadblocks for researchers and slows\ninnovation. These problems are further exacerbated by emerging hybrid operating\nmodels that place Graphical Processing Unit (GPU) supercomputing and Quantum\nIntermediate Representation (QIR) at the heart of real-time computations across\nquantum and distributed resources. There is a need for a widely adopted runtime\nplatform (RP) driven by the open-source community that can be easily deployed\nto work in a distributed manner between Quantum Processing Unit (QPU), GPU,\ncontrol hardware, external compute resources and provide required flexibility\nin terms of programming & configuration models."}
{"arxiv_id": "2409.18020v1", "published": "2024-09-26T16:28:04Z", "title": "Exploring the Dynamics of CME-Driven Shocks by Comparing Numerical\n  Modeling and Observations", "summary": "Shocks driven by coronal mass ejections (CMEs) are primary drivers of gradual\nsolar energetic particle (SEP) events, posing significant risks to space\ntechnology and astronauts. Concurrently, particles accelerated at these shocks\nmay also propagate back to the Sun, potentially generating gamma-ray emissions\nthrough pion decay. We incorporated advanced modeling and multi-messenger\nobservations to explore the role of CME-driven shocks in gamma-ray emissions\nand SEPs. Motivated by Fermi-LAT long-duration solar flares, we used the AWSoM\nMHD model to investigate the connection between the shocks and the properties\nof observed gamma-ray emissions. By coupling the AWSoM with iPATH model, we\nevaluate the impact of shock evolution complexity near the Sun on SEP intensity\nand spectra. Our result points to the importance of accurate background coronal\nand solar wind modeling, as well as detailed observations of CME source\nregions, in advancing our understanding of CME-driven shocks and the dynamics\nof associated energetic particles."}
{"arxiv_id": "2409.17980v1", "published": "2024-09-26T15:53:14Z", "title": "Formal verification of higher dimensional quantum protocols", "summary": "Formal methods have been a successful approach for modelling and verifying\nthe correctness of complex technologies like microprocessor chip design,\nbiological systems and others. This is the main motivation of developing\nquantum formal techniques which is to describe and analyse quantum information\nprocessing systems. Our previous work demonstrates the possibility of using a\nquantum process calculus called Communicating Quantum Processes (CQP) to model\nand describe higher dimensional quantum systems. By developing the theory to\ngeneralise the fundamental gates and Bell states, we have modelled quantum\nqudit protocols like teleportation and superdense coding in CQP. In this paper,\nwe demonstrate the use of CQP to analyse higher dimensional quantum protocols.\nThe main idea is to define two processes, one modelling the real protocol and\nthe other expressing a specification, and prove that they are behaviourally\nequivalent. This is a work-in-progress and we present our preliminary results\nin extending the theory of behavioural equivalence in CQP to verify higher\ndimensional quantum protocols using qudits."}
{"arxiv_id": "2409.17973v1", "published": "2024-09-26T15:47:51Z", "title": "Manufacturing, processing, applications, and advancements of Fe-based\n  shape memory alloys", "summary": "Fe-based shape memory alloys (Fe-SMAs) belong to smart metallic materials\nthat can memorize or restore their preset shape after experiencing a\nsubstantial amount of deformation under heat, stress, or magnetic stimuli.\nFe-SMAs have remarkable thermomechanical properties and have attracted\nsignificant interest because of their potential merits, such as cost-effective\nalloying elements, superior workability, weldability, a stable superelastic\nresponse, and low-temperature dependence of critical stress required for\nstress-induced martensitic transformation. Therefore, Fe-SMAs can be an\nintriguing and economical alternative to other SMAs. The recent advancements in\nfabrication methods of conventional metals and SMAs are helping the production\nof customized powder composition and then customized geometries by additive\nmanufacturing (AM). The technology in these areas, i.e., fabrication\ntechniques, experimental characterization, and theoretical formulations of\nFe-SMAs for conventional and AM has been rapidly advancing and is lacking a\ncomprehensive review. This paper provides a critical review of the recent\ndevelopments in Fe-SMAs-related research. The conventional and AM-based methods\nof producing Fe-SMAs are discussed, and a detailed review of the current\nresearch trends on Fe-SMAs including 4-D printing of Fe-SMAs are\ncomprehensively documented. The presented review provides a comprehensive\nreview of experimental methods and processes used to determine the material\ncharacteristics and features of Fe-SMAs. In addition, the work provides a\nreview of the reported computational modeling of Fe-SMAs to help design new\nFe-SMA composition and geometry. Finally, different Fe-SMAs-based applications\nsuch as sensing and damping systems, tube coupling, and reinforced concrete are\nalso discussed."}
{"arxiv_id": "2409.17959v1", "published": "2024-09-26T15:36:14Z", "title": "A Policy Report Evaluating the National Assessment Program for Literacy\n  and Numeracy (Naplan) Reform in Australia: The Impacts of High Stakes\n  Assessment on Students", "summary": "The National Assessment Program for Literacy and Numeracy (NAPLAN) Reform in\nAustralia, launched in 2008, has emerged as the country's most significant and\ncontentious reform. However, due to its high-stakes nature and standardization,\ntesting presents various challenges. These challenges include the combination\nof accountability with the 'My School' website, overlooking higher-order\ncognitive abilities, exacerbating students' anxiety and stress, and creating\ninequity for Language Background Other Than English (LBOTE) students. This\nreport assesses the achievements and obstacles of the NAPLAN reform, proposing\nrecommendations such as transitioning to online testing, enhancing content and\nplatforms, increasing public assessment literacy, and investing more in LBOTE\neducation. These suggestions aim to strike a balance between standardized\ntesting and authentic educational pursuits, adapting to the evolving needs of\nstudents to create a fair, inclusive educational environment that addresses the\ndemands of the 21st century."}
{"arxiv_id": "2409.17945v1", "published": "2024-09-26T15:20:21Z", "title": "Modular Autonomous Vehicle in Heterogeneous Traffic Flow: Modeling,\n  Simulation, and Implication", "summary": "Modular autonomous vehicles (MAVs) represent a groundbreaking concept that\nintegrates modularity into the ongoing development of autonomous vehicles. This\ninnovative design introduces unique features to traffic flow, allowing multiple\nmodules to seamlessly join together and operate collectively. To understand the\ntraffic flow characteristics involving these vehicles and their collective\noperations, this study established a modeling framework specifically designed\nto simulate their behavior within traffic flow. The mixed traffic flow,\nincorporating arbitrarily formed trains of various modular sizes, is modeled\nand studied. Simulations are conducted under varying levels of traffic demand\nand penetration rates to examine the traffic flow dynamics in the presence of\nthese vehicles and their operations. The microscopic trajectories, MAV train\ncompositions, and macroscopic fundamental diagrams of the mixed traffic flow\nare analyzed. The simulation findings indicate that integrating MAVs and their\ncollective operations can substantially enhance capacity, with the extent of\nimprovement depending on the penetration rate in mixed traffic flow. Notably,\nthe capacity nearly doubles when the penetration rate exceeds 75%. Furthermore,\ntheir presence significantly influences and regulates the free-flow speed of\nthe mixed traffic. Particularly, when variations in operational speed limits\nexist between the MAVs and the background traffic, the mixed traffic adjusts to\nthe operating velocity of these vehicles. This study provides insights into\npotential future traffic flow systems incorporating emerging MAV technologies."}
{"arxiv_id": "2409.17916v1", "published": "2024-09-26T15:02:11Z", "title": "Observer-Based Discontinuous Communication in the Secondary Control of\n  AC Microgrids", "summary": "This paper proposes an observer-based event-driven approach to decrease the\noveruse of communication networks. The suggested approach aims to estimate the\nrequired data for sharing between units in line with as much communication\nreduction as possible. In other words, the proposed approach effectively\ndetermines which state variables should be shared (observer concept) among the\nunits during specific time intervals (event-triggered concept). This strategy\nsignificantly reduces the overall communication load. It is shown that the\nestimation error remains bounded and Zeno behavior, characterized by an endless\nnumber of transmissions occurring within a limited time frame, does not occur.\nThe proposed methodology can be systematically applied to any\ncommunication-based secondary controller in alternating current (AC)\nmicrogrids. Simulation results demonstrate a high degree of precision in\nestimating the states under the proposed approach. Also, the secondary\ncontroller performance under the proposed method is evaluated in\nMATLAB/Simulink environment."}
{"arxiv_id": "2409.17908v1", "published": "2024-09-26T14:52:55Z", "title": "LKA-ReID:Vehicle Re-Identification with Large Kernel Attention", "summary": "With the rapid development of intelligent transportation systems and the\npopularity of smart city infrastructure, Vehicle Re-ID technology has become an\nimportant research field. The vehicle Re-ID task faces an important challenge,\nwhich is the high similarity between different vehicles. Existing methods use\nadditional detection or segmentation models to extract differentiated local\nfeatures. However, these methods either rely on additional annotations or\ngreatly increase the computational cost. Using attention mechanism to capture\nglobal and local features is crucial to solve the challenge of high similarity\nbetween classes in vehicle Re-ID tasks. In this paper, we propose LKA-ReID with\nlarge kernel attention. Specifically, the large kernel attention (LKA) utilizes\nthe advantages of self-attention and also benefits from the advantages of\nconvolution, which can extract the global and local features of the vehicle\nmore comprehensively. We also introduce hybrid channel attention (HCA) combines\nchannel attention with spatial information, so that the model can better focus\non channels and feature regions, and ignore background and other disturbing\ninformation. Experiments on VeRi-776 dataset demonstrated the effectiveness of\nLKA-ReID, with mAP reaches 86.65% and Rank-1 reaches 98.03%."}
{"arxiv_id": "2409.18131v1", "published": "2024-09-26T17:59:57Z", "title": "Two-dopant origin of competing stripe and pair formation in Hubbard and\n  $t$-$J$ models", "summary": "Understanding the physics of the two-dimensional Hubbard model is widely\nbelieved to be a key step in achieving a full understanding of\nhigh-$T_\\mathrm{c}$ cuprate superconductors. In recent years, progress has been\nmade by large-scale numerical simulations at finite doping and, on the other\nhand, by microscopic theories able to capture the physics of individual charge\ncarriers. In this work, we study single pairs of dopants in a cylindrical\nsystem using the density-matrix renormalization group algorithm. We identify\ntwo coexisting charge configurations that couple to the spin environment in\ndifferent ways: A tightly bound configuration featuring (next-)nearest-neighbor\npairs and a stripe-like configuration of dopants on opposite sides of the\ncylinder, accompanied by a spin domain wall. Thus, we establish that the\ninterplay between stripe order and uniform pairing, central to the models'\nphases at finite doping, has its origin at the single-pair level. By\ninterpolating between the Hubbard and the related $t$-$J$ model, we are able to\nquantitatively understand discrepancies in the pairing properties of the two\nmodels through the three-site hopping term usually omitted from the $t$-$J$\nHamiltonian. This term is closely related to a next-nearest-neighbor tunneling\n$t'$, which we observe to upset the balance between the competing stripe and\npair states on the two-dopant level."}
{"arxiv_id": "2409.18130v1", "published": "2024-09-26T17:59:55Z", "title": "Bridging 4D QFTs and 2D VOAs via 3D high-temperature EFTs", "summary": "The high-temperature limit of the superconformal index, especially on higher\nsheets, often captures useful universal information about a theory. In 4d\n$\\mathcal{N}=2$ superconformal field theories with fractional r-charges, there\nexists a special notion of high-temperature limit on higher sheets that\ncaptures data of three-dimensional topological quantum field theories arising\nfrom r-twisted circle reduction. These TQFTs are closely tied with the VOA of\nthe 4d SCFT. We study such high-temperature limits. More specifically, we apply\nDi~Pietro-Komargodski type supersymmetric effective field theory techniques to\nr-twisted circle reductions of $(A_1,A_{2n})$ Argyres-Douglas theories,\nleveraging their Maruyoshi-Song Lagrangian with manifest $\\mathcal{N}=1$\nsupersymmetry. The result on the second sheet is the Gang-Kim-Stubbs family of\n3d $\\mathcal{N}=2$ SUSY enhancing rank-$0$ theories with monopole\nsuperpotentials, whose boundary supports the Virasoro minimal model VOAs\n$M(2,2n+3)$. Upon topological twist, they give non-unitary TQFTs controlled by\nthe $M(2,2n+3)$ modular tensor category (MTC). The high-temperature limit on\nother sheets yields their unitary or non-unitary Galois conjugates. This opens\nup the prospect of a broader four-supercharge perspective on the celebrated\ncorrespondence between 4d $\\mathcal{N}=2$ SCFTs and 2d VOAs via interpolating\n3d EFTs. Several byproducts follow, including a systematic approach to 3d SUSY\nenhancement from 4d SUSY enhancement, and a 3d QFT handle on Galois orbits of\nvarious MTCs associated with 4d $\\mathcal{N}=2$ SCFTs."}
{"arxiv_id": "2409.18128v1", "published": "2024-09-26T17:59:51Z", "title": "FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity\n  Refiner", "summary": "Building on the success of diffusion models in visual generation, flow-based\nmodels reemerge as another prominent family of generative models that have\nachieved competitive or better performance in terms of both visual quality and\ninference speed. By learning the velocity field through flow-matching,\nflow-based models tend to produce a straighter sampling trajectory, which is\nadvantageous during the sampling process. However, unlike diffusion models for\nwhich fast samplers are well-developed, efficient sampling of flow-based\ngenerative models has been rarely explored. In this paper, we propose a\nframework called FlowTurbo to accelerate the sampling of flow-based models\nwhile still enhancing the sampling quality. Our primary observation is that the\nvelocity predictor's outputs in the flow-based models will become stable during\nthe sampling, enabling the estimation of velocity via a lightweight velocity\nrefiner. Additionally, we introduce several techniques including a pseudo\ncorrector and sample-aware compilation to further reduce inference time. Since\nFlowTurbo does not change the multi-step sampling paradigm, it can be\neffectively applied for various tasks such as image editing, inpainting, etc.\nBy integrating FlowTurbo into different flow-based models, we obtain an\nacceleration ratio of 53.1%$\\sim$58.3% on class-conditional generation and\n29.8%$\\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID\nof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),\nachieving the real-time image generation and establishing the new\nstate-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo."}
{"arxiv_id": "2409.18127v1", "published": "2024-09-26T17:59:31Z", "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions", "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning."}
{"arxiv_id": "2409.18126v1", "published": "2024-09-26T17:59:12Z", "title": "Boltzmann Sampling by Diabatic Quantum Annealing", "summary": "It has been proposed that diabatic quantum annealing (DQA), which turns off\nthe transverse field at a finite speed, produces samples well described by the\nBoltzmann distribution. We analytically show that, up to linear order in\nquenching time, the DQA approximates a high-temperature Boltzmann distribution.\nOur theory yields an explicit relation between the quenching rate of the DQA\nand the temperature of the Boltzmann distribution. Based on this result, we\ndiscuss how the DQA can be utilized to train the Restricted Boltzmann Machine\n(RBM)."}
{"arxiv_id": "2409.18108v1", "published": "2024-09-26T17:51:31Z", "title": "Language-Embedded Gaussian Splats (LEGS): Incrementally Building\n  Room-Scale Representations with a Mobile Robot", "summary": "Building semantic 3D maps is valuable for searching for objects of interest\nin offices, warehouses, stores, and homes. We present a mapping system that\nincrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D\nscene representation that encodes both appearance and semantics in a unified\nrepresentation. LEGS is trained online as a robot traverses its environment to\nenable localization of open-vocabulary object queries. We evaluate LEGS on 4\nroom-scale scenes where we query for objects in the scene to assess how LEGS\ncan capture semantic meaning. We compare LEGS to LERF and find that while both\nsystems have comparable object query success rates, LEGS trains over 3.5x\nfaster than LERF. Results suggest that a multi-camera setup and incremental\nbundle adjustment can boost visual reconstruction quality in constrained robot\ntrajectories, and suggest LEGS can localize open-vocabulary and long-tail\nobject queries with up to 66% accuracy."}
{"arxiv_id": "2409.18102v1", "published": "2024-09-26T17:45:10Z", "title": "MALPOLON: A Framework for Deep Species Distribution Modeling", "summary": "This paper describes a deep-SDM framework, MALPOLON. Written in Python and\nbuilt upon the PyTorch library, this framework aims to facilitate training and\ninferences of deep species distribution models (deep-SDM) and sharing for users\nwith only general Python language skills (e.g., modeling ecologists) who are\ninterested in testing deep learning approaches to build new SDMs. More advanced\nusers can also benefit from the framework's modularity to run more specific\nexperiments by overriding existing classes while taking advantage of\npress-button examples to train neural networks on multiple classification tasks\nusing custom or provided raw and pre-processed datasets. The framework is\nopen-sourced on GitHub and PyPi along with extensive documentation and examples\nof use in various scenarios. MALPOLON offers straightforward installation,\nYAML-based configuration, parallel computing, multi-GPU utilization, baseline\nand foundational models for benchmarking, and extensive\ntutorials/documentation, aiming to enhance accessibility and performance\nscalability for ecologists and researchers."}
{"arxiv_id": "2409.18097v1", "published": "2024-09-26T17:41:04Z", "title": "A Sim-to-Real Vision-based Lane Keeping System for a 1:10-scale\n  Autonomous Vehicle", "summary": "In recent years, several competitions have highlighted the need to\ninvestigate vision-based solutions to address scenarios with functional\ninsufficiencies in perception, world modeling and localization. This article\npresents the Vision-based Lane Keeping System (VbLKS) developed by the\nDEI-Unipd Team within the context of the Bosch Future Mobility Challenge 2022.\nThe main contribution lies in a Simulation-to-Reality (Sim2Real) GPS-denied\nVbLKS for a 1:10-scale autonomous vehicle. In this VbLKS, the input to a\ntailored Pure Pursuit (PP) based control strategy, namely the Lookahead Heading\nError (LHE), is estimated at a constant lookahead distance employing a\nConvolutional Neural Network (CNN). A training strategy for a compact CNN is\nproposed, emphasizing data generation and augmentation on simulated camera\nimages from a 3D Gazebo simulator, and enabling real-time operation on\nlow-level hardware. A tailored PP-based lateral controller equipped with a\nderivative action and a PP-based velocity reference generation are implemented.\nTuning ranges are established through a systematic time-delay stability\nanalysis. Validation in a representative controlled laboratory setting is\nprovided."}
{"arxiv_id": "2409.18094v1", "published": "2024-09-26T17:40:00Z", "title": "Mobility in Age-Based Gossip Networks", "summary": "We consider a gossiping network where a source forwards updates to a set of\n$n$ gossiping nodes that are placed in an arbitrary graph structure and gossip\nwith their neighbors. In this paper, we analyze how mobility of nodes affects\nthe freshness of nodes in the gossiping network. To model mobility, we let\nnodes randomly exchange positions with other nodes in the network. The position\nof the node determines how the node interacts with the rest of the network. In\norder to quantify information freshness, we use the version age of information\nmetric. We use the stochastic hybrid system (SHS) framework to derive recursive\nequations to find the version age for a set of positions in the network in\nterms of the version ages of sets of positions that are one larger or of the\nsame size. We use these recursive equations to find an upper bound for the\naverage version age of a node in two example networks. We show that mobility\ncan decrease the version age of nodes in a disconnected network from linear\nscaling in $n$ to at most square root scaling and even to constant scaling in\nsome cases. We perform numerical simulations to analyze how mobility affects\nthe version age of different positions in the network and also show that the\nupper bounds obtained for the example networks are tight."}
{"arxiv_id": "2409.18089v1", "published": "2024-09-26T17:34:46Z", "title": "Some contributions to sheaf model theory", "summary": "This paper makes contributions to \"pure\" sheaf model theory, the part of\nmodel theory in which the models are sheaves over a complete Heyting algebra.\nWe start by outlining the theory in a way we hope is readable for the\nnon-specialist. We then give a careful treatment of the interpretation of terms\nand formulae. This allows us to prove various preservation results, including\nstrengthenings of the results of Brunner and Miraglia. We give refinements of\nMiraglia's work on directed colimits and an analogue of Tarski's theorem on the\npreservation of $\\forall_2$-sentences under unions of chains. We next show\nvarious categories whose objects are (pairs of) presheaves and sheaves with\nvarious notions of morphism are accessible in the category theoretic sense.\nTogether these ingredients allow us ultimately to prove that these categories\nare encompassed in the AECats framework for independence relations developed by\nKamsma."}
{"arxiv_id": "2409.18084v1", "published": "2024-09-26T17:27:15Z", "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal\n  Model", "summary": "As the number of service robots and autonomous vehicles in human-centered\nenvironments grows, their requirements go beyond simply navigating to a\ndestination. They must also take into account dynamic social contexts and\nensure respect and comfort for others in shared spaces, which poses significant\nchallenges for perception and planning. In this paper, we present a group-based\nsocial navigation framework GSON to enable mobile robots to perceive and\nexploit the social group of their surroundings by leveling the visual reasoning\ncapability of the Large Multimodal Model (LMM). For perception, we apply visual\nprompting techniques to zero-shot extract the social relationship among\npedestrians and combine the result with a robust pedestrian detection and\ntracking pipeline to alleviate the problem of low inference speed of the LMM.\nGiven the perception result, the planning system is designed to avoid\ndisrupting the current social structure. We adopt a social structure-based\nmid-level planner as a bridge between global path planning and local motion\nplanning to preserve the global context and reactive response. The proposed\nmethod is validated on real-world mobile robot navigation tasks involving\ncomplex social structure understanding and reasoning. Experimental results\ndemonstrate the effectiveness of the system in these scenarios compared with\nseveral baselines."}
{"arxiv_id": "2409.18064v1", "published": "2024-09-26T17:05:32Z", "title": "A scaling law of the neutral opacity and Balmer-$α$ wing shape in\n  high-temperature plasmas", "summary": "Hydrogen atoms penetrating deep inside high-temperature magnetically confined\nplasmas by repetitive charge-exchange collisions result in a particle source,\nwhich affects the plasma performance significantly. In this \\textit{Letter}, we\npresent an approximate solution of the fluid equations for neutral transport\nand the analytical representation of the neutral opacity, in a simplified\nplasma geometry. This analysis predicts a power-law decay in the\nBalmer-$\\alpha$ line wings which reflects the velocity distribution of the\nneutral atoms, with the power-law index analytically represented as well. These\nscaling laws are validated by the comparison with a simple Monte-Carlo\nsimulation and spectroscopic observations of Large Helical Device plasmas.\nSince the Balmer-$\\alpha$ line wings are experimentally accessible, our\nformulation opens the door to directly observe the neutral opacity and thus the\nparticle source distribution in the plasma."}
{"arxiv_id": "2409.18060v1", "published": "2024-09-26T17:01:33Z", "title": "Infering Alt-text For UI Icons With Large Language Models During App\n  Development", "summary": "Ensuring accessibility in mobile applications remains a significant\nchallenge, particularly for visually impaired users who rely on screen readers.\nUser interface icons are essential for navigation and interaction and often\nlack meaningful alt-text, creating barriers to effective use. Traditional deep\nlearning approaches for generating alt-text require extensive datasets and\nstruggle with the diversity and imbalance of icon types. More recent Vision\nLanguage Models (VLMs) require complete UI screens, which can be impractical\nduring the iterative phases of app development. To address these issues, we\nintroduce a novel method using Large Language Models (LLMs) to autonomously\ngenerate informative alt-text for mobile UI icons with partial UI data. By\nincorporating icon context, that include class, resource ID, bounds,\nOCR-detected text, and contextual information from parent and sibling nodes, we\nfine-tune an off-the-shelf LLM on a small dataset of approximately 1.4k icons,\nyielding IconDesc. In an empirical evaluation and a user study IconDesc\ndemonstrates significant improvements in generating relevant alt-text. This\nability makes IconDesc an invaluable tool for developers, aiding in the rapid\niteration and enhancement of UI accessibility."}
{"arxiv_id": "2409.18048v1", "published": "2024-09-26T16:49:57Z", "title": "Next-Gen Software Engineering: AI-Assisted Big Models", "summary": "The effectiveness of model-driven software engineering (MDSE) has been\ndemonstrated in the context of complex software; however, it has not been\nwidely adopted due to the requisite efforts associated with model development\nand maintenance, as well as the specific modelling competencies required for\nMDSE. Concurrently, artificial intelligence (AI) methods, particularly machine\nlearning (ML) methods, have demonstrated considerable abilities when applied to\nthe huge code bases accessible on open-source coding platforms. The so-called\nbig code provides the basis for significant advances in empirical software\nengineering, as well as in the automation of coding processes and improvements\nin software quality with the use of AI. The objective of this paper is to\nfacilitate a synthesis between these two significant domains of software\nengineering (SE), namely models and AI in SE. The paper provides an overview of\nthe current status of AI-assisted software engineering. In light of the\naforementioned considerations, a vision of AI-assisted Big Models in SE is put\nforth, with the aim of capitalising on the advantages inherent to both\napproaches in the context of software development. Finally, the new paradigm of\npair modelling in MDSE is proposed."}
{"arxiv_id": "2409.18039v1", "published": "2024-09-26T16:43:07Z", "title": "Ecosystem-Agnostic Standardization of Quantum Runtime Architecture:\n  Accelerating Utility in Quantum Computing", "summary": "Fault tolerance is a long-term objective driving many companies and research\norganizations to compete in making current, imperfect quantum computers useful\n- Quantum Utility (QU). It looks promising to achieve this by leveraging\nsoftware optimization approaches primarily driven by AI techniques. This\naggressive research covers all layers of Quantum Computing Optimization\nMiddleware (QCOM) and requires execution on real quantum hardware (QH). Due to\nthe nascent nature of the technology domain and the proprietary strategies of\nboth large and small players, popular runtimes for executing quantum workloads\nlack flexibility in programming models, scheduling, and hardware access\npatterns, including queuing, which creates roadblocks for researchers and slows\ninnovation. These problems are further exacerbated by emerging hybrid operating\nmodels that place Graphical Processing Unit (GPU) supercomputing and Quantum\nIntermediate Representation (QIR) at the heart of real-time computations across\nquantum and distributed resources. There is a need for a widely adopted runtime\nplatform (RP) driven by the open-source community that can be easily deployed\nto work in a distributed manner between Quantum Processing Unit (QPU), GPU,\ncontrol hardware, external compute resources and provide required flexibility\nin terms of programming & configuration models."}
{"arxiv_id": "2409.18108v1", "published": "2024-09-26T17:51:31Z", "title": "Language-Embedded Gaussian Splats (LEGS): Incrementally Building\n  Room-Scale Representations with a Mobile Robot", "summary": "Building semantic 3D maps is valuable for searching for objects of interest\nin offices, warehouses, stores, and homes. We present a mapping system that\nincrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D\nscene representation that encodes both appearance and semantics in a unified\nrepresentation. LEGS is trained online as a robot traverses its environment to\nenable localization of open-vocabulary object queries. We evaluate LEGS on 4\nroom-scale scenes where we query for objects in the scene to assess how LEGS\ncan capture semantic meaning. We compare LEGS to LERF and find that while both\nsystems have comparable object query success rates, LEGS trains over 3.5x\nfaster than LERF. Results suggest that a multi-camera setup and incremental\nbundle adjustment can boost visual reconstruction quality in constrained robot\ntrajectories, and suggest LEGS can localize open-vocabulary and long-tail\nobject queries with up to 66% accuracy."}
{"arxiv_id": "2409.18097v1", "published": "2024-09-26T17:41:04Z", "title": "A Sim-to-Real Vision-based Lane Keeping System for a 1:10-scale\n  Autonomous Vehicle", "summary": "In recent years, several competitions have highlighted the need to\ninvestigate vision-based solutions to address scenarios with functional\ninsufficiencies in perception, world modeling and localization. This article\npresents the Vision-based Lane Keeping System (VbLKS) developed by the\nDEI-Unipd Team within the context of the Bosch Future Mobility Challenge 2022.\nThe main contribution lies in a Simulation-to-Reality (Sim2Real) GPS-denied\nVbLKS for a 1:10-scale autonomous vehicle. In this VbLKS, the input to a\ntailored Pure Pursuit (PP) based control strategy, namely the Lookahead Heading\nError (LHE), is estimated at a constant lookahead distance employing a\nConvolutional Neural Network (CNN). A training strategy for a compact CNN is\nproposed, emphasizing data generation and augmentation on simulated camera\nimages from a 3D Gazebo simulator, and enabling real-time operation on\nlow-level hardware. A tailored PP-based lateral controller equipped with a\nderivative action and a PP-based velocity reference generation are implemented.\nTuning ranges are established through a systematic time-delay stability\nanalysis. Validation in a representative controlled laboratory setting is\nprovided."}
{"arxiv_id": "2409.18094v1", "published": "2024-09-26T17:40:00Z", "title": "Mobility in Age-Based Gossip Networks", "summary": "We consider a gossiping network where a source forwards updates to a set of\n$n$ gossiping nodes that are placed in an arbitrary graph structure and gossip\nwith their neighbors. In this paper, we analyze how mobility of nodes affects\nthe freshness of nodes in the gossiping network. To model mobility, we let\nnodes randomly exchange positions with other nodes in the network. The position\nof the node determines how the node interacts with the rest of the network. In\norder to quantify information freshness, we use the version age of information\nmetric. We use the stochastic hybrid system (SHS) framework to derive recursive\nequations to find the version age for a set of positions in the network in\nterms of the version ages of sets of positions that are one larger or of the\nsame size. We use these recursive equations to find an upper bound for the\naverage version age of a node in two example networks. We show that mobility\ncan decrease the version age of nodes in a disconnected network from linear\nscaling in $n$ to at most square root scaling and even to constant scaling in\nsome cases. We perform numerical simulations to analyze how mobility affects\nthe version age of different positions in the network and also show that the\nupper bounds obtained for the example networks are tight."}
{"arxiv_id": "2409.18084v1", "published": "2024-09-26T17:27:15Z", "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal\n  Model", "summary": "As the number of service robots and autonomous vehicles in human-centered\nenvironments grows, their requirements go beyond simply navigating to a\ndestination. They must also take into account dynamic social contexts and\nensure respect and comfort for others in shared spaces, which poses significant\nchallenges for perception and planning. In this paper, we present a group-based\nsocial navigation framework GSON to enable mobile robots to perceive and\nexploit the social group of their surroundings by leveling the visual reasoning\ncapability of the Large Multimodal Model (LMM). For perception, we apply visual\nprompting techniques to zero-shot extract the social relationship among\npedestrians and combine the result with a robust pedestrian detection and\ntracking pipeline to alleviate the problem of low inference speed of the LMM.\nGiven the perception result, the planning system is designed to avoid\ndisrupting the current social structure. We adopt a social structure-based\nmid-level planner as a bridge between global path planning and local motion\nplanning to preserve the global context and reactive response. The proposed\nmethod is validated on real-world mobile robot navigation tasks involving\ncomplex social structure understanding and reasoning. Experimental results\ndemonstrate the effectiveness of the system in these scenarios compared with\nseveral baselines."}
{"arxiv_id": "2409.18060v1", "published": "2024-09-26T17:01:33Z", "title": "Infering Alt-text For UI Icons With Large Language Models During App\n  Development", "summary": "Ensuring accessibility in mobile applications remains a significant\nchallenge, particularly for visually impaired users who rely on screen readers.\nUser interface icons are essential for navigation and interaction and often\nlack meaningful alt-text, creating barriers to effective use. Traditional deep\nlearning approaches for generating alt-text require extensive datasets and\nstruggle with the diversity and imbalance of icon types. More recent Vision\nLanguage Models (VLMs) require complete UI screens, which can be impractical\nduring the iterative phases of app development. To address these issues, we\nintroduce a novel method using Large Language Models (LLMs) to autonomously\ngenerate informative alt-text for mobile UI icons with partial UI data. By\nincorporating icon context, that include class, resource ID, bounds,\nOCR-detected text, and contextual information from parent and sibling nodes, we\nfine-tune an off-the-shelf LLM on a small dataset of approximately 1.4k icons,\nyielding IconDesc. In an empirical evaluation and a user study IconDesc\ndemonstrates significant improvements in generating relevant alt-text. This\nability makes IconDesc an invaluable tool for developers, aiding in the rapid\niteration and enhancement of UI accessibility."}
{"arxiv_id": "2409.18028v1", "published": "2024-09-26T16:34:35Z", "title": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective", "summary": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically."}
{"arxiv_id": "2409.17985v1", "published": "2024-09-26T15:55:59Z", "title": "Hypergame Theory for Decentralized Resource Allocation in Multi-user\n  Semantic Communications", "summary": "Semantic communications (SC) is an emerging communication paradigm in which\nwireless devices can send only relevant information from a source of data while\nrelying on computing resources to regenerate missing data points. However, the\ndesign of a multi-user SC system becomes more challenging because of the\ncomputing and communication overhead required for coordination. Existing\nsolutions for learning the semantic language and performing resource allocation\noften fail to capture the computing and communication tradeoffs involved in\nmultiuser SC. To address this gap, a novel framework for decentralized\ncomputing and communication resource allocation in multiuser SC systems is\nproposed. The challenge of efficiently allocating communication and computing\nresources (for reasoning) in a decentralized manner to maximize the quality of\ntask experience for the end users is addressed through the application of\nStackelberg hyper game theory. Leveraging the concept of second-level hyper\ngames, novel analytical formulations are developed to model misperceptions of\nthe users about each other's communication and control strategies. Further,\nequilibrium analysis of the learned resource allocation protocols examines the\nconvergence of the computing and communication strategies to a local\nStackelberg equilibria, considering misperceptions. Simulation results show\nthat the proposed Stackelberg hyper game results in efficient usage of\ncommunication and computing resources while maintaining a high quality of\nexperience for the users compared to state-of-the-art that does not account for\nthe misperceptions."}
{"arxiv_id": "2409.17978v1", "published": "2024-09-26T15:52:36Z", "title": "HydraViT: Stacking Heads for a Scalable ViT", "summary": "The architecture of Vision Transformers (ViTs), particularly the Multi-head\nAttention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs\non devices with varying constraints, such as mobile phones, requires multiple\nmodels of different sizes. However, this approach has limitations, such as\ntraining and storing each required model separately. This paper introduces\nHydraViT, a novel approach that addresses these limitations by stacking\nattention heads to achieve a scalable ViT. By repeatedly changing the size of\nthe embedded dimensions throughout each layer and their corresponding number of\nattention heads in MHA during training, HydraViT induces multiple subnetworks.\nThereby, HydraViT achieves adaptability across a wide spectrum of hardware\nenvironments while maintaining performance. Our experimental results\ndemonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10\nsubnetworks, covering a wide range of resource constraints. HydraViT achieves\nup to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy\nwith the same throughput on ImageNet-1K compared to the baselines, making it an\neffective solution for scenarios where hardware availability is diverse or\nvaries over time. Source code available at https://github.com/ds-kiel/HydraViT."}
{"arxiv_id": "2409.17924v1", "published": "2024-09-26T15:05:29Z", "title": "Neural Light Spheres for Implicit Image Stitching and View Synthesis", "summary": "Challenging to capture, and challenging to display on a cellphone screen, the\npanorama paradoxically remains both a staple and underused feature of modern\nmobile camera applications. In this work we address both of these challenges\nwith a spherical neural light field model for implicit panoramic image\nstitching and re-rendering; able to accommodate for depth parallax,\nview-dependent lighting, and local scene motion and color changes during\ncapture. Fit during test-time to an arbitrary path panoramic video capture --\nvertical, horizontal, random-walk -- these neural light spheres jointly\nestimate the camera path and a high-resolution scene reconstruction to produce\nnovel wide field-of-view projections of the environment. Our single-layer model\navoids expensive volumetric sampling, and decomposes the scene into compact\nview-dependent ray offset and color components, with a total model size of 80\nMB per scene, and real-time (50 FPS) rendering at 1080p resolution. We\ndemonstrate improved reconstruction quality over traditional image stitching\nand radiance field methods, with significantly higher tolerance to scene motion\nand non-ideal capture settings."}
{"arxiv_id": "2409.17882v1", "published": "2024-09-26T14:29:46Z", "title": "Multi-UAV Enabled MEC Networks: Optimizing Delay through Intelligent 3D\n  Trajectory Planning and Resource Allocation", "summary": "Mobile Edge Computing (MEC) reduces the computational burden on terminal\ndevices by shortening the distance between these devices and computing nodes.\nIntegrating Unmanned Aerial Vehicles (UAVs) with enhanced MEC networks can\nleverage the high mobility of UAVs to flexibly adjust network topology, further\nexpanding the applicability of MEC. However, in highly dynamic and complex\nreal-world environments, it is crucial to balance task offloading effectiveness\nwith algorithm performance. This paper investigates a multi-UAV communication\nnetwork equipped with edge computing nodes to assist terminal users in task\ncomputation. Our goal is to reduce the task processing delay for users through\nthe joint optimization of discrete computation modes, continuous 3D\ntrajectories, and resource assignment. To address the challenges posed by the\nmixed action space, we propose a Multi-UAV Edge Computing Resource Scheduling\n(MUECRS) algorithm, which comprises two key components: 1) trajectory\noptimization, and 2) computation mode and resource management. Experimental\nresults demonstrate our method effectively designs the 3D flight trajectories\nof UAVs, enabling rapid terminal coverage. Furthermore, the proposed algorithm\nachieves efficient resource deployment and scheduling, outperforming\ncomparative algorithms by at least 16.7%, demonstrating superior adaptability\nand robustness."}
